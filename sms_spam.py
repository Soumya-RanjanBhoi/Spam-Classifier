# -*- coding: utf-8 -*-
"""SMS_SPAM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1383zjql1n8yDDKMXFoTLshf3mVUrl4d5
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from nltk import word_tokenize,sent_tokenize
from nltk.corpus import stopwords
import spacy

import torch
torch.cuda.is_available()

!nvidia-smi

df=pd.read_csv("/content/spam.csv", encoding='latin-1')

df.sample(10)

df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)

df.rename({"v2":"message","v1":"label"},axis=1,inplace=True)

df['message'].loc[0]

df.info()

df.shape

df.isnull().sum()

df[df.duplicated()]

df.drop_duplicates(subset='message',keep='first',inplace=True)

df['label'].value_counts()

plt.bar(df['label'].value_counts().index, df['label'].value_counts().values)
plt.xlabel('Label')
plt.ylabel('Count')
plt.title('Distribution of Spam and Ham')
plt.show()

"""#Text Preprocessing"""

import regex as re

def remove_html(text):
    if isinstance(text,str):
        pattern=re.compile("<.*?>")
        return pattern.sub(" ",text)
    return text

df['message']=df['message'].apply(remove_html)

def remove_url(text, replacement=""):
    url_pattern = re.compile(r'https?://\S+|www\.\S+', re.MULTILINE)
    return url_pattern.sub(replacement, text)

df['message']=df['message'].apply(remove_url)

df['total_len']=df['message'].apply(len)

df.head()

def lowering(text):
    return text.lower()

import nltk
nltk.download('punkt_tab')

df['word_count']= df['message'].apply(lambda x : len(word_tokenize(x)))

df['sent_count']=df['message'].apply(lambda x :len(sent_tokenize(x)) )

df['message']=df['message'].apply(lowering)

df['label']=df['label'].map({"ham":0, "spam":1})

import string
punctuation=string.punctuation

def remove_punc(text):
    if isinstance(text,str):
        return "".join([char for char in text if char not  in punctuation])
    return text

df['message']=df['message'].apply(remove_punc)

import nltk
nltk.download('stopwords')

stopwords.words("english")

from nltk.stem import WordNetLemmatizer

lemm=WordNetLemmatizer()

def transformation(text):
    if isinstance(text, str):
        nlp = spacy.load("en_core_web_sm")
        doc = nlp(text)
        tokens = word_tokenize(str(doc))

        words = []
        for token in tokens:
            token_clean = token.replace(".", "")
            if token_clean.isalnum() and token.lower() not in stopwords.words("english"):
                words.append(token)
        return words
    return text

transformation("Hello World20 ** is a soumya an U.S A.I")

df['message_tokens']= df['message'].apply(transformation)

df.head()

df.to_csv('spam_1.csv')

"""`conclusion:`

more the length of sentence - Spam
"""

df1=pd.read_csv("/content/spam_1.csv")

df1.head(10)

df1.drop("Unnamed: 0",axis=1,inplace=True)

df1[df1['label']==1].shape

"""#pre processing"""

from nltk.stem.porter import PorterStemmer
import ast
ps=PorterStemmer()

def stemming(text):
    if isinstance(text, str):
        try:
            text = ast.literal_eval(text)
        except:
            text = text.split()
    new_sent = [ps.stem(word) for word in text]
    return new_sent

df1['message_tokens']=df1['message_tokens'].apply(stemming)

df1.head(10)

def appending(text):
   return  " ".join([char for char in text])

import nltk
nltk.download('punkt_tab')

nltk.download('stopwords')

import string
punctuation=string.punctuation

def transform_text(text):
    text = text.lower()
    text = nltk.word_tokenize(text)

    y = []
    for i in text:
        if i.isalnum():
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:
        y.append(lemm.lemmatize(i))


    return " ".join(y)

from tqdm.auto import tqdm
tqdm.pandas()

nltk.download('wordnet')

df1['transformed_text']=df1['message'].progress_apply(transform_text)

df1.sample(10)

df1[df1['label']==1]

"""##analysis"""

from wordcloud import WordCloud
wc=WordCloud(width=500,height=500,min_font_size=10,background_color='white')

spam_wc = wc.generate(df1[df1['label'] == 1]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(15,6))
plt.imshow(spam_wc)

not_spam_wc=wc.generate(df1[df1['label']==0]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(10, 5))
plt.imshow(not_spam_wc)
plt.axis("off")
plt.title("Not Spam WordCloud")
plt.show()

sns.histplot(df[df['label']==0]['total_len'], color='red', label='Not-Spam', kde=False)
sns.histplot(df[df['label']==1]['total_len'], color='blue', label='Spam', kde=False)
plt.legend()
plt.show()

sns.histplot(df[df['label']==0]['word_count'], color='red', label='Not-Spam', kde=False)
sns.histplot(df[df['label']==1]['word_count'], color='blue', label='Spam', kde=False)
plt.legend()
plt.show()

spam_corpus=[]
for text in df1[df1['label']==1]['transformed_text'].to_list():
    for word in text.split():
        spam_corpus.append(word)

not_spam_corpus=[]
for text in df1[df1['label']==0]['transformed_text'].to_list():
    for word in text.split():
        not_spam_corpus.append(word)

from collections import Counter
spam_most_text=Counter(spam_corpus).most_common(50)

not_spam_text=Counter(not_spam_corpus).most_common(50)

spam_most_text

sns.barplot(x=pd.DataFrame(Counter(spam_corpus).most_common(50))[0],y=pd.DataFrame(Counter(spam_corpus).most_common(50))[1])
plt.xticks(rotation='vertical')
plt.show()

sns.barplot(x=pd.DataFrame(Counter(not_spam_corpus).most_common(50))[0],y=pd.DataFrame(Counter(not_spam_corpus).most_common(50))[1])
plt.xticks(rotation='vertical')
plt.show()

"""#Modelling"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

"""1.n_grams"""

cv=CountVectorizer(ngram_range=(1,5))

X_ngrams=cv.fit_transform(df1['transformed_text']).toarray()

X_ngrams.shape

y=df1['label']

tdif=TfidfVectorizer()

tdif1=TfidfVectorizer(max_features=3000)

X_tdif=tdif.fit_transform(df1['transformed_text']).toarray()

X_tdif1=tdif1.fit_transform(df1['transformed_text']).toarray()

df1.columns

!pip install gensim

import gensim
from nltk import sent_tokenize
from gensim.utils import simple_preprocess

model=gensim.models.Word2Vec(vector_size=100,window=5)

story=[]
for i in range(df1.shape[0]):

    story.append(simple_preprocess(df1.iloc[i]['transformed_text']))

story[0]

model.build_vocab(story)

model.train(story, total_examples=model.corpus_count, epochs=100)

model.wv.most_similar('point')

df1.shape

model.corpus_count

import numpy as np

def documentation(doc):
    x = [word  for word in doc.split() if word in model.wv.index_to_key ]
    if not x:
        return np.zeros(model.vector_size)
    else:
        return np.mean(model.wv[x],axis=0)

documentation(df1['transformed_text'].values[0])

X_w2v=df1['transformed_text'].progress_apply(documentation)

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score as sk_accuracy_score, f1_score as sk_f1_score, precision_score as sk_precision_score
from sklearn.base import clone
import numpy as np

def try_model(model, X, y):
    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

    try:
        model1 = clone(model)
    except AttributeError:
        model1 = model

    model1.fit(x_train, y_train)

    y_pred = model1.predict(x_test)

    acc_score = sk_accuracy_score(y_test, y_pred)
    f1 = sk_f1_score(y_test, y_pred)
    precision = sk_precision_score(y_test, y_pred, average='weighted')

    return {"accracy":acc_score,"f1":f1,"Precision":precision}

try_model(GaussianNB(),np.array(list(X_w2v)),y)

try_model(GaussianNB(),np.array(X_tdif),y)

for model in [GaussianNB(),MultinomialNB(),BernoulliNB()]:
    for X_name, X in [('X_tdif', X_tdif), ('X_w2v', list(X_w2v))]:
        if isinstance(model, MultinomialNB) and X_name == 'X_w2v':
            print(f"Skipping MultinomialNB with {X_name} due to negative values.")
            print()
            continue
        try_model(model,np.array(X),y)
        print(X_name)
        print()

!pip install xgboost

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

svc = SVC(kernel='sigmoid', gamma=1.0)
knc = KNeighborsClassifier()
mnb = MultinomialNB()
gnv=GaussianNB()
bnv=BernoulliNB()
dtc = DecisionTreeClassifier(max_depth=5)
lrc = LogisticRegression(solver='liblinear', penalty='l1')
rfc = RandomForestClassifier(n_estimators=50, random_state=2)
abc = AdaBoostClassifier(n_estimators=50, random_state=2)
bc = BaggingClassifier(n_estimators=50, random_state=2)
etc = ExtraTreesClassifier(n_estimators=50, random_state=2)
gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)
xgb = XGBClassifier(n_estimators=100,random_state=2)

clfs = {
    'SVC' : svc,
    'KN' : knc,
    "GuasssianNv":gnv,
    "BernoulliNV":bnv,
    'DicisionTree': dtc,
    'LogisticR': lrc,
    'RandomForest': rfc,
    'AdaBoost': abc,
    'BaggingC': bc,
    'ETC': etc,
    'GBDT':gbdt,
    'xgb':xgb
}

def train_classifier(clf,X,y=y):
    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)
    clf.fit(X_train,y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test,y_pred)
    precision = precision_score(y_test,y_pred)

    return accuracy,precision

train_classifier(svc,X_tdif,y)

accuracy_scores=[]
f1_scores=[]
precision_scores=[]
X_used=[]
clf_used=[]

for name,clf in clfs.items():
    for X_name, X in [('X_tdif', X_tdif), ('X_w2v', list(X_w2v)),('X_tdf1',X_tdif1)]:
        info=try_model(clf,X,y)
        clf_used.append(name)
        accuracy_scores.append(info['accracy'])
        f1_scores.append(info['f1'])
        precision_scores.append(info['Precision'])
        X_used.append(X_name)

final=pd.DataFrame({"Classifier Used":clf_used,"X_used":X_used,"Accuracy":accuracy_scores,"F1":f1_scores,"precision":precision_scores})

final

final[final['precision']==max(final['precision'])]

final[final['Accuracy']==max(final['Accuracy'])]

final['Accuracy'].sort_values(ascending=False)[0:3]

final['precision'].sort_values(ascending=False)[0:3]

from sklearn.ensemble import StackingClassifier

#KN
#BernalliNV
#xgb
#ETC

estimators=[("KN",KNeighborsClassifier()),('BernalliNV',BernoulliNB()),("Xgb",XGBClassifier(n_estimators=100)),("ETC",ExtraTreesClassifier(n_estimators=100))]

final_estimator=RandomForestClassifier(n_estimators=100)

final_model = StackingClassifier(estimators=estimators, final_estimator=final_estimator)

from sklearn.metrics import accuracy_score, precision_score
import numpy as np

for X_name, X in [("X_tdif", X_tdif), ("X_w2v", np.array(list(X_w2v))), ("X_tdif1", X_tdif1)]:
    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    final_model.fit(x_train, y_train)
    y_pred = final_model.predict(x_test)

    print("X -->", X_name)
    print("Accuracy -->", accuracy_score(y_test, y_pred))
    print("Precision -->", precision_score(y_test, y_pred))
    print()

accuracy_score(y_test,final_model.predict(x_test))

X_train,X_test,y_train,y_test=train_test_split(X_tdif1,y,test_size=0.2,random_state=42)

final_model1 = StackingClassifier(estimators=estimators, final_estimator=final_estimator)

final_model1.fit(X_train,y_train)

accuracy_score(y_test,final_model1.predict(X_test))

precision_score(y_test, final_model1.predict(X_test))

import pickle
pickle.dump(tdif1,open('vectorizer1.pkl','wb'))

with open("vectorizer.pkl", "rb") as f:
    v = pickle.load(f)

# v.transform(['"Hey, are we still meeting for lunch tomorrow?"'])